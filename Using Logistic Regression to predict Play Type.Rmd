---
title: "Using Machine Learning to Predict the Outcome of a Play in the National Football League"
output: html_document
date: "2023-07-02"
---

```{r}
library(sjlabelled)
library(labelled)
library(janitor)
library(nflfastR)
library(tidyverse)
```


I will be using supervised learning algorithms to predict the categorical outcome of a play in the National Football League using 2009 to 2023 play-by-play NFL data provided by nflfastR. Over time I see the future of the NFL to have data scientists with fast computing machines to predict real time the next play. Would this ruin the game? Maybe. But I am confident that football, a sport that is dear to my heart, will become more exciting as time goes on. 


Avid football fans can tell what the play will be based on the down, yards to go and positioning of the players. 


Let's first start by loading the data
```{r}
pbp <- load_pbp(c(2020:2022)) 
```

```{r}
data <- pbp %>%
  select(game_id, desc, home_team, away_team,season_type, play_type,ydstogo, qtr, down, game_seconds_remaining,
         yardline_100, yrdln, drive, season, season_type, away_score, home_score, rush_attempt, pass_attempt) %>%
   #dplyr::filter(season_type == "POST" & 
    #             home_team == "MIN" & 
     #            away_team == "NYG") %>%
  filter(rush_attempt == 1 | pass_attempt == 1) %>%
  filter(play_type == "pass" | play_type == "run") %>%
  mutate(game_state = case_when(away_score > home_score ~ 0,
                                away_score < home_score ~ 1,
                                away_score == home_score ~2 )) %>%
  set_value_labels(game_state = c("Away Team Up" = 0,
                                     "Home Team Up" = 1,
                                     "Tie" = 2))
# want only pass and rush attempts  

```

Let's run a logistic regression 
```{r}
data_logreg <- data %>%
  mutate(play_type_factor = recode(play_type,
    "pass" = 1,
    "run" = 0
  )) %>%
   mutate(play_type_factor = as.factor(play_type_factor), #outcome variable
         qtr_factor = as.factor(qtr),
         down_factor = as.factor(down)
  ) 
```

create training data
```{r}
library(caret)
Train <- createDataPartition(data_logreg$play_type_factor, p=.6, list = FALSE)
training <- data_logreg[Train,]
testing <- data_logreg[-Train,]
```


```{r}
mylogit <- glm(play_type_factor ~ qtr_factor + down_factor+ drive + ydstogo + game_seconds_remaining,
               family = "binomial",
               data = data_logreg)
```

```{r}
summary(mylogit)
```

3rd quarter makes it more likely to rush (makes sense when you usually teams will only have a few yards to go to make the down)

```{r}
exp(coef(mylogit))
```

```{r}
confint(mylogit)
```

Let's look at the predicted probailities and graph them with their standard errors to produce a confidence interval 
```{r}
newdata <- predict(mylogit, 
                        newdata = testing,
                        type = "link",
                        se = TRUE)

#i get an error. so now i need to make the make them a dataframe
df1<- data.frame(matrix(unlist(newdata$fit), ncol = 1 , byrow = TRUE))
df2 <- data.frame(matrix(unlist(newdata$se.fit), ncol = 1 , byrow = TRUE))
df3 <- data.frame(matrix(unlist(newdata$residual.scale), ncol = 1 , byrow = TRUE))
df4 <- bind_cols(df1,df2,df3)

colnames(df4)[1] = "fit"
colnames(df4)[2] = "se.fit"
colnames(df4)[3] = "residual.scale"

newdata3 <- cbind(testing, df4)
newdata3 <- within(newdata3, {
    PredictedProb <- plogis(fit)
    LL <- plogis(fit - (1.96 * se.fit))
    UL <- plogis(fit + (1.96 * se.fit))
})

## view first few rows of final dataset
```

Looks like the predicted probability goes down as the yards to go increases
The predicted probability is highest with the first down 
```{r}
newdata3 %>%
  drop_na(down_factor) %>%
ggplot(aes(x = ydstogo , y = PredictedProb)) + 
  geom_ribbon(aes(ymin = LL, ymax = UL, fill = down_factor), alpha = 0.2) + 
  geom_line(aes(colour = down_factor), 
            size = 1)


```
Confusion Matrix

```{r}
library(caret)
pred <- predict(mylogit,
                     testing,
                     type ="response")
# If p exceeds threshold of 0.5, 1 else 0
play_type_pred <- ifelse(pred > 0.5, 1, 0)

# Convert to factor: p_class
p_class <- factor(play_type_pred, levels = levels(testing[["play_type_factor"]]))



accuracy <- table(p_class, testing$play_type_factor)
sum(diag(accuracy))/sum(accuracy)

confusionMatrix(data = p_class,  #predicted classes
                reference = testing$play_type_factor) #true results ) 

```

The accuracy of our predicted model is 63% ! Not bad but we can do better. 
Let's try Naive Bayes, KNN, Decision Tree, Random Forest, & Kernal Support Vector Machine next. 




